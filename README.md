# nlp_toolkit

ä¸­æ–‡NLPåŸºç¡€å·¥å…·ç®±ï¼ŒåŒ…æ‹¬ä»¥ä¸‹ä»»åŠ¡ï¼šä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ç­‰ã€‚

æœ¬ä»“åº“å¤ç°äº†ä¸€äº›è¿‘å‡ å¹´æ¯”è¾ƒç«çš„nlpè®ºæ–‡ã€‚æ‰€æœ‰çš„ä»£ç æ˜¯åŸºäºkeraså¼€å‘çš„ã€‚

ä¸åˆ°10è¡Œä»£ç ï¼Œä½ å°±å¯ä»¥å¿«é€Ÿè®­ç»ƒä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼ˆæš‚æ—¶ä¸æ”¯æŒå¤šæ ‡ç­¾ä»»åŠ¡ï¼‰æˆ–åºåˆ—æ ‡æ³¨æ¨¡å‹ï¼Œæˆ–è€…å¯ä»¥ä½“éªŒåŸºäºåè¯çŸ­è¯­åˆ‡åˆ†çš„åˆ†è¯å™¨

## ç›´æ¥å®‰è£…

```bash
pip install nlp_toolkit

# ä½¿ç”¨GPU
pip install tensorflow-gpu, GPUtil
```

## æ‰‹åŠ¨å®‰è£…

```bash
git clone https://github.com/stevewyl/nlp_toolkit
cd nlp_toolkit

# åªä½¿ç”¨CPU
pip install -r requirements.txt

# ä½¿ç”¨GPU
pip install -r requirements-gpu.txt

# å¦‚æœkeras_contribå®‰è£…å¤±è´¥
pip install git+https://www.github.com/keras-team/keras-contrib.git
```

### å®‰è£…é”™è¯¯

1. ImportError: cannot import name 'normalize_data_format'

    ```bash
    pip install -U keras
    ```

## ä½¿ç”¨æ–¹æ³•

æœ¬ä»“åº“çš„æ¡†æ¶å›¾ï¼š

![framework](./images/framework.jpg)

ä¸»è¦ç”±ä»¥ä¸‹å‡ å¤§æ¨¡å—ç»„æˆï¼š

1. Datasetï¼šå¤„ç†æ–‡æœ¬å’Œæ ‡ç­¾æ•°æ®ä¸ºé€‚åˆæ¨¡å‹è¾“å…¥çš„æ ¼å¼ï¼Œä¸»è¦è¿›è¡Œçš„å¤„ç†æ“ä½œæœ‰æ¸…ç†ã€åˆ†è¯ã€indexåŒ–

2. Model Zoo & Layerï¼šè¿‘å‡ å¹´åœ¨è¯¥ä»»åŠ¡ä¸­å¸¸ç”¨çš„æ¨¡å‹æ±‡æ€»åŠä¸€äº›Kerasçš„è‡ªå®šä¹‰å±‚

   ç›®å‰æ”¯æŒçš„è‡ªå®šä¹‰å±‚æœ‰å¦‚ä¸‹ï¼š

   * 1Dæ³¨æ„åŠ›å±‚ ğŸ†—
   * 2Dæ³¨æ„åŠ›å±‚ ğŸ†—
   * å¤šå¤´æ³¨æ„åŠ›å±‚ ğŸ†—
   * ä½ç½®åµŒå…¥å±‚ ğŸ†—
   * K-maxæ± åŒ–å±‚

3. Trainerï¼šå®šä¹‰æ¨¡å‹çš„è®­ç»ƒæµç¨‹ï¼Œæ”¯æŒbucketåºåˆ—ã€è‡ªå®šä¹‰callbackså’ŒNæŠ˜äº¤å‰éªŒè¯

    * bucketåºåˆ—ï¼šé€šè¿‡å°†ç›¸ä¼¼é•¿åº¦çš„æ–‡æœ¬æ”¾å…¥åŒä¸€batchæ¥å‡å°paddingçš„å¤šä½™è®¡ç®—æ¥å®ç°æ¨¡å‹è®­ç»ƒçš„åŠ é€Ÿï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œèƒ½å¤Ÿå¯¹RNNç½‘ç»œæé€Ÿ2å€ä»¥ä¸Šï¼ˆ**æš‚æ—¶ä¸æ”¯æŒå«æœ‰Flattenå±‚çš„ç½‘ç»œ**ï¼‰
  
    * callbacksï¼šé€šè¿‡è‡ªå®šä¹‰å›è°ƒå™¨æ¥æ§åˆ¶è®­ç»ƒæµç¨‹ï¼Œç›®å‰é¢„è®¾çš„å›è°ƒå™¨æœ‰æå‰ç»ˆæ­¢è®­ç»ƒï¼Œå­¦ä¹ ç‡è‡ªåŠ¨å˜åŒ–ï¼Œæ›´ä¸°å¯Œçš„è¯„ä¼°å‡½æ•°ç­‰

    * NæŠ˜äº¤å‰éªŒè¯ï¼šæ”¯æŒäº¤å‰éªŒè¯æ¥è€ƒéªŒæ¨¡å‹çš„çœŸå®èƒ½åŠ›

4. Classifier & Sequence Labelerï¼šå°è£…ç±»ï¼Œæ”¯æŒä¸åŒçš„è®­ç»ƒä»»åŠ¡

5. Applicationï¼šç›®å‰å·¥å…·ç®±å†…å°è£…äº†åŸºäºjiebaçš„åè¯çŸ­è¯­åˆ†è¯å™¨ Chunk_Segmentor (å¦‚éœ€æ¨¡å‹æ–‡ä»¶ï¼Œå¯ä»¥é‚®ä»¶è”ç³»æˆ‘)

ç®€å•çš„ç”¨æ³•å¦‚ä¸‹ï¼š

```python
from nlp_toolkit import Dataset, Classifier, Labeler
import yaml

config = yaml.load(open('your_config.yaml'))

# åˆ†ç±»ä»»åŠ¡
dataset = Dataset(fname='your_data.txt', task_type='classification', mode='train', config=config)
text_classifier = Classifier('multi_head_self_att', dataset)
trained_model = text_classifier.train()

# åºåˆ—æ ‡æ³¨ä»»åŠ¡
dataset = Dataset(fname='your_data.txt', task_type='sequence_labeling', mode='train', config=config)
seq_labeler = Labeler('word_rnn', dataset)
trained_model = seq_labeler.train()

# é¢„æµ‹ï¼ˆä»¥æ–‡æœ¬åˆ†ç±»ä¸ºä¾‹ï¼‰
dataset = Dataset(fname='your_data.txt', task_type='classification', mode='predict', tran_fname='your_transformer.h5')
text_classifier = Classifier('bi_lstm_att', dataset)
text_classifier.load(weight_fname='your_model_weights.h5', para_fname='your_model_parameters.json')
y_pred = text_classifier.predict(dataset.texts)

# chunkåˆ†è¯
# ç¬¬ä¸€æ¬¡importçš„æ—¶å€™ï¼Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹å’Œå­—å…¸æ•°æ®
# æ”¯æŒå•å¥å’Œå¤šå¥æ–‡æœ¬çš„è¾“å…¥æ ¼å¼ï¼Œå»ºè®®ä»¥åˆ—è¡¨çš„å½¢å¼ä¼ å…¥åˆ†è¯å™¨
# æºä»£ç ä¸­å·²ç•¥å»ç›¸å…³æ•°æ®çš„ä¸‹è½½è·¯å¾„ï¼Œæœ‰éœ€è¦çš„è¯·é‚®ä»¶è”ç³»
from nlp_toolkit.chunk_segmentor import Chunk_Segmentor
cutter = Chunk_Segmentor()
s = 'è¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿè¾“å‡ºåè¯çŸ­è¯­çš„åˆ†è¯å™¨ï¼Œæ¬¢è¿è¯•ç”¨ï¼'
res = [item for item in cutter.cut([s] * 10000)] # 1080tiä¸Šè€—æ—¶8s
# æä¾›ä¸¤ä¸ªç‰ˆæœ¬ï¼Œaccurateä¸ºç²¾ç¡®ç‰ˆï¼Œfastä¸ºå¿«é€Ÿç‰ˆä½†å¬å›ä¼šé™ä½ä¸€äº›ï¼Œé»˜è®¤ç²¾ç¡®ç‰ˆ
cutter = Chunk_Segmentor(mode='accurate')
cutter = Chunk_Segmentor(mode='fast')
# æ˜¯å¦è¾“å‡ºè¯æ€§ï¼Œ é»˜è®¤å¼€å¯
cutter.cut(s, pos=False)
# æ˜¯å¦å°†å¯åˆ‡åˆ†çš„åè¯çŸ­è¯­åˆ‡åˆ†ï¼Œé»˜è®¤å…³é—­
cutter.cut(s, cut_all=True)
# è¾“å‡ºæ ¼å¼ï¼ˆè¯åˆ—è¡¨ï¼Œè¯æ€§åˆ—è¡¨ï¼Œåè¯çŸ­è¯­é›†åˆï¼‰
[
    (
        ['è¿™', 'æ˜¯', 'ä¸€ä¸ª', 'èƒ½å¤Ÿ', 'è¾“å‡º', 'åè¯_çŸ­è¯­', 'çš„', 'åˆ†è¯å™¨', ',', 'æ¬¢è¿', 'è¯•ç”¨', '!'],
        ['r', 'v', 'mq', 'v', 'vn', 'np', 'ude1', 'np', 'w', 'v', 'v', 'w'],
        ['åˆ†è¯å™¨', 'åè¯_çŸ­è¯­']
    )
    ...
]
```

æ›´å¤šä½¿ç”¨ç»†èŠ‚ï¼Œè¯·é˜…è¯»[**examples**](https://github.com/stevewyl/nlp_toolkit/tree/master/examples)æ–‡ä»¶å¤¹ä¸­çš„Jupyter Notebookå’Œchunk_segmentoré¡µé¢çš„[**README**](https://github.com/stevewyl/nlp_toolkit/tree/master/nlp_toolkit/chunk_segmentor)

### æ•°æ®æ ¼å¼

1. æ–‡æœ¬åˆ†ç±»ï¼šæ¯ä¸€è¡Œé¢„å…ˆåˆ†å¥½è¯çš„æ–‡ä»¶ï¼Œæ¯ä¸€è¡Œçš„æ ¼å¼å¦‚ä¸‹ï¼š

    __label__æ ‡ç­¾1 __label__æ ‡ç­¾2 ... è¯ è¯ ... è¯\n

    ä¾‹å¦‚ â€œ__label__neg å…¬å¸ ç›®å‰ åœ°ç† ä½ç½® ä¸ å¤ª ç†æƒ³ ï¼Œ ç¦» åŸå¸‚ ä¸­å¿ƒ è¾ƒ è¿œç‚¹ ã€‚â€

2. åºåˆ—æ ‡æ³¨ï¼šæ¯ä¸€è¡Œé¢„å…ˆåˆ†å¥½è¯çš„æ–‡ä»¶ï¼Œæ”¯æŒä¸¤ç§æ•°æ®æ ¼å¼ï¼Œæ¯ä¸€è¡Œçš„æ ¼å¼å¦‚ä¸‹ï¼š

    è¯###æ ‡ç­¾ [TAB] è¯###æ ‡ç­¾ [TAB] ... \n

    ä¾‹å¦‚ â€œç›®å‰###O\tå…¬å¸###O\tåœ°ç†###B-Chunk\tä½ç½®###E-Chunk\tä¸###O\tå¤ª###O\tç†æƒ³\nâ€

    æˆ–è€… CONLLçš„æ ‡å‡†æ ¼å¼

    è¯ [TAB] æ ‡ç­¾

    è¯ [TAB] æ ‡ç­¾

    ...

    è¯ [TAB] æ ‡ç­¾

    è¯ [TAB] æ ‡ç­¾

    ...

    ä¾‹å¦‚ï¼š

    ç›®å‰\tO

    å…¬å¸\tO

    ...

    åœ°ç†\tB-Chunk

    ä½ç½®\tE-Chunk

    ä¸\tO

    å¤ª\tO

    ç†æƒ³\tO

    æ ‡ç­¾å«ä¹‰ï¼ˆè¿™é‡Œä»¥chunkä¸ºä¾‹ï¼‰ï¼š

    * Oï¼šæ™®é€šè¯
    * B-Chunkï¼šè¡¨ç¤ºchunkè¯çš„å¼€å§‹
    * I-Chunkï¼šè¡¨ç¤ºchunkè¯çš„ä¸­é—´
    * E-Chunkï¼šè¡¨ç¤ºchunkè¯çš„ç»“æŸ

    å»ºè®®ï¼šæ–‡æœ¬åºåˆ—ä»¥çŸ­å¥ä¸ºä¸»ï¼Œé’ˆå¯¹æ ‡æ³¨å®ä½“çš„ä»»åŠ¡ï¼Œæœ€å¥½ä¿è¯æ¯è¡Œæ•°æ®ä¸­æœ‰å®ä½“è¯ï¼ˆå³éå…¨Oçš„åºåˆ—ï¼‰

    ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼äº’ç›¸è½¬æ¢ä¸¤ç§æ•°æ®æ ¼å¼ï¼š
    ```python
    from nlp_toolkit.utilities import convert_seq_format
    # here we convert dataset from conll format to basic format
    convert_seq_format(input_file, output_file, 'basic')
    ```

    ps: å…·ä½“å¯æŸ¥çœ‹dataæ–‡ä»¶å¤¹ä¸­å¯¹åº”çš„[**ç¤ºä¾‹æ•°æ®**](https://github.com/stevewyl/nlp_toolkit/tree/master/sample_data)

3. é¢„æµ‹ï¼šä¸åŒä»»åŠ¡æ¯ä¸€è¡Œå‡ä¸ºé¢„å…ˆåˆ†å¥½è¯çš„æ–‡æœ¬åºåˆ—

4. æ”¯æŒç®€å•çš„è‡ªå·±æ·»åŠ æ•°æ®çš„æ–¹æ³•

   ```python
   dataset = Dataset(task_type='classification', mode='train', config=config)
   # classification
   dataset.add({'text': 'æˆ‘ çˆ± æœºå™¨ å­¦ä¹ ', 'label': 'pos'})
   # sequence labeling
   dataset.add({'text': 'æˆ‘ çˆ± æœºå™¨ å­¦ä¹ ', 'label': 'O O B-Chunk E-Chunk'})
   # after you add all your data
   dataset.fit()
   ```

### é…ç½®æ–‡ä»¶

nlp_toolkité€šè¿‡é…ç½®æ–‡ä»¶æ¥åˆå§‹åŒ–è®­ç»ƒä»»åŠ¡

train: è¡¨ç¤ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„å‚æ•°ï¼ŒåŒ…æ‹¬batchå¤§å°ï¼Œepochæ•°é‡ï¼Œè®­ç»ƒæ¨¡å¼ç­‰

data: è¡¨ç¤ºæ•°æ®é¢„å¤„ç†çš„å‚æ•°ï¼ŒåŒ…æ‹¬æœ€å¤§è¯æ•°å’Œå­—ç¬¦æ•°ï¼Œæ˜¯å¦ä½¿ç”¨è¯å†…éƒ¨å­—ç¬¦åºåˆ—ç­‰

embed: è¯å‘é‡ï¼Œpreè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡

å‰©ä¸‹çš„æ¨¡å—å¯¹åº”ä¸åŒçš„æ¨¡å‹çš„è¶…å‚æ•°

å…·ä½“ç»†èŠ‚å¯æŸ¥çœ‹ä»“åº“æ ¹ç›®å½•ä¸‹çš„ä¸¤ä¸ª**é…ç½®æ–‡ä»¶**æ³¨é‡Š

### å¯è§†åŒ–

1. attentionæƒé‡å¯è§†åŒ–

    ```python
    # only support model bi_lstm_att currently
    # first you need to get attention_weights from model predictions
    # you can find the actual usage in examples/sentiment.ipynb
    texts = 'æœ‰ èƒ½åŠ› çš„ äºº å°± æœ‰ å¾ˆå¤š æœºä¼š'
    from nlp_toolkit import visualization as vs
    vs.mk_html(texts, attention_weights)
    ```

    <span style="background-color: #FFFAFA">æœ‰</span> <span style="background-color: #FFB6B6">èƒ½åŠ›</span> <span style="background-color: #FFFBFB">çš„</span> <span style="background-color: #FFF8F8">äºº</span> <span style="background-color: #FFEFEF">å°±</span> <span style="background-color: #FFE3E3">æœ‰</span> <span style="background-color: #FFEFEF">å¾ˆå¤š</span> <span style="background-color: #FF9191">æœºä¼š</span>

2. å®ä½“é¢„æµ‹ç»“æœå¯è§†åŒ–

   ```python
   from nlp_toolkit import visualization as vs
   vs.entity_visualization(dataset.texts, y_pred, output_fname='result.html')
   ```

3. acc/loss æ›²çº¿å¯è§†åŒ–

   ```python
   # after your have trained one model, you will also get a history object, which contains some loss and metrics info
   from nlp_toolkit import visualization as vs
   vs.plot_loss_acc(history, task='sequence_labeling')
   ```

### å…¶ä»–

1. ç”Ÿæˆè¯å‘é‡å°æ–‡ä»¶

    ```python
    from nlp_toolkit.utilities import gen_small_embedding
    gen_small_embedding(vocab_file, embed_file, output_file)
    ```

## æ¨¡å‹

### æ–‡æœ¬åˆ†ç±»

1. åŒå±‚åŒå‘LSTM + Attention ğŸ†—

    [DeepMoji](https://arxiv.org/abs/1708.00524)ä¸€æ–‡ä¸­æ‰€é‡‡ç”¨çš„çš„æ¨¡å‹æ¡†æ¶ï¼Œæœ¬ä»“åº“ä¸­å¯¹attentionå±‚ä½œäº†æ‰©å±•

    å¯¹åº”é…ç½®æ–‡ä»¶ä¸­çš„åç§°ï¼šbi_lstm_att

2. [Transformer](http://papers.nips.cc/paper/7181-attention-is-all-you-need) ğŸ†—

    é‡‡ç”¨Transformerä¸­çš„å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚æ¥è¡¨å¾æ–‡æœ¬ä¿¡æ¯ï¼Œè¯¦ç»†çš„ç»†èŠ‚å¯é˜…è¯»æ­¤[æ–‡ç« ](https://kexue.fm/archives/4765)

    å¯¹åº”é…ç½®æ–‡ä»¶ä¸­çš„åç§°ï¼šmulti_head_self_att

3. [TextCNN](https://arxiv.org/abs/1408.5882) ğŸ†—

    CNNç½‘ç»œä¹‹äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„å¼€å±±ä¹‹ä½œï¼Œåœ¨è¿‡å»å‡ å¹´ä¸­ç»å¸¸è¢«ç”¨ä½œbaselineï¼Œè¯¦ç»†çš„ç»†èŠ‚å¯é˜…è¯»æ­¤[æ–‡ç« ](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)

    å¯¹åº”é…ç½®æ–‡ä»¶ä¸­çš„åç§°ï¼štext_cnn

4. [DPCNN](http://www.aclweb.org/anthology/P17-1052) ğŸ†—

    åœ¨textCNNçš„åŸºç¡€ä¸Šï¼ŒDPCNNä½¿ç”¨æ®‹å·®è¿æ¥ã€å›ºå®šfeature mapæ•°é‡å’Œ1/2æ± åŒ–å±‚ç­‰æŠ€å·§æ¥å®ç°æ›´ä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤ºï¼Œè¯¦ç»†çš„ç»†èŠ‚å¯é˜…è¯»æ­¤[æ–‡ç« ](https://zhuanlan.zhihu.com/p/35457093)

    å¯¹åº”é…ç½®æ–‡ä»¶ä¸­çš„åç§°ï¼šdpcnn
    æš‚æ—¶ä¸æ”¯æŒbucketåºåˆ—åŒ–çš„æ•°æ®

5. [HAN](https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf)

    ä½¿ç”¨attentionæœºåˆ¶çš„æ–‡æ¡£åˆ†ç±»æ¨¡å‹

### åºåˆ—æ ‡æ³¨

1. [WordRNN](https://arxiv.org/abs/1707.06799) ğŸ†—

    Baselineæ¨¡å‹ï¼Œæ–‡æœ¬åºåˆ—ç»è¿‡åŒå‘LSTMåï¼Œç”±CRFå±‚ç¼–ç ä½œä¸ºè¾“å‡º

    å¯¹åº”é…ç½®æ–‡ä»¶ä¸­çš„åç§°ï¼šword_rnn

2. [CharRNN](https://pdfs.semanticscholar.org/b944/5206f592423f0b2faf05f99de124ccc6aaa8.pdf) ğŸ†—

    åŸºäºæ±‰è¯­çš„ç‰¹ç‚¹ï¼Œåœ¨å­—ç¬¦çº§åˆ«çš„LSTMä¿¡æ¯å¤–ï¼ŒåŠ å…¥åæ—éƒ¨é¦–ï¼Œåˆ†è¯ï¼ŒNgramä¿¡æ¯

3. [InnerChar](https://arxiv.org/abs/1611.04361) ğŸ†—

    åŸºäºå¦å¤–ä¸€ç¯‡[è®ºæ–‡](https://arxiv.org/abs/1511.08308)ï¼Œæ‰©å±•äº†æœ¬æ–‡çš„æ¨¡å‹ï¼Œä½¿ç”¨bi-lstmæˆ–CNNåœ¨è¯å†…éƒ¨çš„charçº§åˆ«è¿›è¡Œä¿¡æ¯çš„æŠ½å–ï¼Œç„¶åä¸åŸæ¥çš„è¯å‘é‡è¿›è¡Œconcatæˆ–attentionè®¡ç®—

    å¯¹åº”é…ç½®æ–‡ä»¶ä¸­çš„åç§°ï¼šword_rnnï¼Œå¹¶è®¾ç½®é…ç½®æ–‡ä»¶dataæ¨¡å—ä¸­çš„inner_charä¸ºTrue

4. [IDCNN](https://arxiv.org/abs/1702.02098) ğŸ†—

    è†¨èƒ€å·ç§¯ç½‘ç»œï¼Œåœ¨ä¿æŒå‚æ•°é‡ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå¢å¤§äº†å·ç§¯æ ¸çš„æ„Ÿå—é‡ï¼Œè¯¦ç»†çš„ç»†èŠ‚å¯é˜…è¯»æ­¤[æ–‡ç« ](http://www.crownpku.com//2017/08/26/%E7%94%A8IDCNN%E5%92%8CCRF%E5%81%9A%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E4%B8%AD%E6%96%87%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB.html)

    å¯¹åº”é…ç½®æ–‡ä»¶ä¸­çš„åç§°ï¼šidcnn

## æ€§èƒ½

åç»­åŠ å…¥å¯¹ä¸­æ–‡NLPçš„æ ‡å‡†æ•°æ®é›†çš„æµ‹è¯•

### æ–‡æœ¬åˆ†ç±»

æµ‹è¯•æ•°æ®é›†ï¼š

1. å…¬å¸ä¼˜ç¼ºç‚¹è¯„ä»·ï¼ŒäºŒåˆ†ç±»ï¼Œæ•°æ®è§„æ¨¡ï¼š95K

    Model                   | 10-fold_f1   | Model Size   | Time per epoch
    ----------------------- | :------:     | :----------: | :-------------:
    Bi-LSTM Attention       |              |              |
    Transformer             |              | 7M           | 12s
    TextCNN                 | 96.57        | 10M          | 19s
    DPCNN                   | 93.35        | 9M           | 28s
    HAN                     |              |              |

### åºåˆ—æ ‡æ³¨

æµ‹è¯•æ•°æ®é›†ï¼š

1. ç®€å†å·¥ä½œç»å†ï¼Œchunkï¼Œæ•°æ®è§„æ¨¡ï¼š58K

    Model                   | 10-fold_f1   | Model Size   | Time per epoch
    ----------------------- | :------:     | :----------: | :-------------:
    Baseline(WordRNN)       |              |              |
    WordRNN + InnerChar     |              | 3M           | 165s
    CharRNN(seg+radical)    |              |              |
    IDCNN                   |              | 2.7M         | 43s

ps: æ¨¡å‹å¤§å°è¡¨ç¤ºä¸ºæ¨¡å‹çš„å‚æ•°é‡ï¼Œå…¶ä¸­Kè¡¨ç¤ºåƒï¼ŒMè¡¨ç¤ºç™¾ä¸‡ï¼›æµ‹è¯•è®¾å¤‡ä¸º1080ti+i7-6800K

## To-Doåˆ—è¡¨

1. åŠ å…¥æ›´å¤šSOTAçš„æ¨¡å‹å’Œè‡ªå®šä¹‰å±‚

2. ä¸‹ä¸€ç‰ˆæœ¬è§„åˆ’ï¼šå¢åŠ æŠ½è±¡ç±»Sentence

3. V2.0è§„åˆ’ï¼šåˆ‡æ¢ä¸ºtf.estimatorå’Œtf.kerasçš„API

## æ„Ÿè°¢

* æ•°æ®æµæ¨¡å—éƒ¨åˆ†ä»£ç å€Ÿé‰´äºæ­¤ï¼š https://github.com/Hironsan/anago/

* åºåˆ—æ ‡æ³¨ä»»åŠ¡çš„è¯„ä¼°å‡½æ•°æ¥æºäºæ­¤ï¼š https://github.com/chakki-works/seqeval
  
* bucketåºåˆ—åŒ–ä»£ç æ¥è‡ªï¼šhttps://github.com/tbennun/keras-bucketed-sequence

* å¤šå¤´æ³¨æ„åŠ›å±‚å’Œä½ç½®åµŒå…¥å±‚ä»£ç æ¥è‡ªï¼šhttps://github.com/bojone/attention

## è”ç³»æ–¹å¼

è”ç³»äººï¼šç‹å¥•ç£Š

ğŸ“§ é‚®ç®±ï¼šstevewyl@163.com

å¾®ä¿¡ï¼šSteve_1125
