# nlp_toolkit

ä¸­æ–‡NLPåŸºç¡€å·¥å…·ç®±ï¼ŒåŒ…æ‹¬ä»¥ä¸‹ä»»åŠ¡ï¼šä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ç­‰ã€‚

æœ¬ä»“åº“å¤ç°äº†ä¸€äº›è¿‘å‡ å¹´æ¯”è¾ƒç«çš„nlpè®ºæ–‡ã€‚æ‰€æœ‰çš„ä»£ç æ˜¯åŸºäºkeraså¼€å‘çš„ã€‚

ä¸åˆ°10è¡Œä»£ç ï¼Œä½ å°±å¯ä»¥å¿«é€Ÿè®­ç»ƒä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ¨¡å‹æˆ–åºåˆ—æ ‡æ³¨æ¨¡å‹ã€‚

## å®‰è£…

```bash
git clone https://github.com/stevewyl/nlp_toolkit
cd nlp_toolkit

# åªä½¿ç”¨CPU
pip install -r requirements.txt

# ä½¿ç”¨GPU
pip install -r requirements-gpu.txt

# å¦‚æœkeras_contribå®‰è£…å¤±è´¥
pip install git+https://www.github.com/keras-team/keras-contrib.git
```

## ä½¿ç”¨æ–¹æ³•

æœ¬ä»“åº“çš„æ¡†æ¶å›¾ï¼š

![framework](./images/framework.jpg)

ä¸»è¦ç”±ä»¥ä¸‹å‡ å¤§æ¨¡å—ç»„æˆï¼š

1. Datasetï¼šå¤„ç†æ–‡æœ¬å’Œæ ‡ç­¾æ•°æ®ä¸ºé€‚åˆæ¨¡å‹è¾“å…¥çš„æ ¼å¼ï¼Œä¸»è¦è¿›è¡Œçš„å¤„ç†æ“ä½œæœ‰æ¸…ç†ã€åˆ†è¯ã€indexåŒ–

2. Model Zoo & Layerï¼šè¿‘å‡ å¹´åœ¨è¯¥ä»»åŠ¡ä¸­å¸¸ç”¨çš„æ¨¡å‹æ±‡æ€»åŠä¸€äº›Kerasçš„è‡ªå®šä¹‰å±‚
   
   è‡ªå®šä¹‰å±‚æœ‰å¦‚ä¸‹ï¼š

   * é€šç”¨æ³¨æ„åŠ›å±‚
  
   * å¤šå¤´æ³¨æ„åŠ›å±‚

   * ä½ç½®åµŒå…¥å±‚

3. Trainerï¼šå®šä¹‰æ¨¡å‹çš„è®­ç»ƒæµç¨‹ï¼Œæ”¯æŒbucketåºåˆ—ã€è‡ªå®šä¹‰callbackså’ŒNæŠ˜äº¤å‰éªŒè¯

    * bucketåºåˆ—ï¼šé€šè¿‡å°†ç›¸ä¼¼é•¿åº¦çš„æ–‡æœ¬æ”¾å…¥åŒä¸€batchæ¥å‡å°paddingçš„å¤šä½™è®¡ç®—æ¥å®ç°æ¨¡å‹è®­ç»ƒçš„åŠ é€Ÿï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œèƒ½å¤Ÿå¯¹RNNç½‘ç»œæé€Ÿ2å€ä»¥ä¸Šï¼ˆæš‚æ—¶ä¸æ”¯æŒå«æœ‰Flattenå±‚çš„ç½‘ç»œï¼‰
  
    * callbacksï¼šé€šè¿‡è‡ªå®šä¹‰å›è°ƒå™¨æ¥æ§åˆ¶è®­ç»ƒæµç¨‹ï¼Œç›®å‰é¢„è®¾çš„å›è°ƒå™¨æœ‰æå‰ç»ˆæ­¢è®­ç»ƒï¼Œå­¦ä¹ ç‡è‡ªåŠ¨å˜åŒ–ï¼Œæ›´ä¸°å¯Œçš„è¯„ä¼°å‡½æ•°ç­‰

    * NæŠ˜äº¤å‰éªŒè¯ï¼šæ”¯æŒäº¤å‰éªŒè¯æ¥è€ƒéªŒæ¨¡å‹çš„çœŸå®èƒ½åŠ›

4. Classifier & Sequence Labelerï¼šå°è£…ç±»ï¼Œæ”¯æŒä¸åŒçš„è®­ç»ƒä»»åŠ¡

ç®€å•çš„ç”¨æ³•å¦‚ä¸‹ï¼š

```python
from nlp_toolkit import Dataset, Classifier, Labeler
import yaml

config = yaml.load(open('your_config.yaml'))

# åˆ†ç±»ä»»åŠ¡
dataset = Dataset(fname='your_data.txt', task_type='classification', mode='train', config=config)
x, y, new_config = dataset.transform()
text_classifier = Classifier(config=new_config, model_name='multi_head_self_att', seq_type='bucket', transformer=dataset.transformer)
trained_model = text_classifier.train(x, y)

# åºåˆ—æ ‡æ³¨ä»»åŠ¡
dataset = Dataset(fname='your_data.txt', task_type='seq_label', mode='train', config=config)
x, y, new_config = dataset.transform()
seq_labeler = Labeler(config=new_config, model_name='word_rnn', seq_type='bucket',transformer=dataset.transformer)
trained_model = seq_labeler.train(x, y)

# é¢„æµ‹
```

æ›´å¤šä½¿ç”¨ç»†èŠ‚ï¼Œè¯·é˜…è¯»**examples**æ–‡ä»¶å¤¹ä¸­çš„Jupyter Notebook

### æ•°æ®æ ¼å¼

1. æ–‡æœ¬åˆ†ç±»ï¼šæ¯ä¸€è¡Œä¸ºæ–‡æœ¬+æ ‡ç­¾ï¼Œ\tåˆ†å‰²ï¼ˆæš‚æ—¶ä¸æ”¯æŒå¤šæ ‡ç­¾ä»»åŠ¡ï¼‰

    ä¾‹å¦‚ â€œå…¬å¸ç›®å‰åœ°ç†ä½ç½®ä¸å¤ªç†æƒ³ï¼Œ ç¦»åŸå¸‚ä¸­å¿ƒè¾ƒè¿œç‚¹ã€‚\tnegâ€

2. åºåˆ—æ ‡æ³¨ï¼šæ¯ä¸€è¡Œä¸ºæ–‡æœ¬+æ ‡ç­¾ï¼Œ\tåˆ†å‰²ï¼Œæ–‡æœ¬åºåˆ—å’Œæ ‡ç­¾åºåˆ—ä¸€ä¸€å¯¹åº”ï¼Œä»¥ç©ºæ ¼åˆ†å‰²

    ä¾‹å¦‚ â€œç›®å‰ å…¬å¸ åœ°ç† ä½ç½® ä¸ å¤ª ç†æƒ³\tO O B-Chunk I-Chunk O O Oâ€

    æ ‡ç­¾å«ä¹‰ï¼ˆè¿™é‡Œä»¥chunkä¸ºä¾‹ï¼‰ï¼š

    * Oï¼šæ™®é€šè¯
    * B-Chunkï¼šè¡¨ç¤ºchunkè¯çš„å¼€å§‹
    * I-Chunkï¼šè¡¨ç¤ºchunkè¯çš„ä¸­é—´
    * E-Chunkï¼šè¡¨ç¤ºchunkè¯çš„ç»“æŸ

    å»ºè®®ï¼šæ–‡æœ¬åºåˆ—ä»¥çŸ­å¥ä¸ºä¸»ï¼Œé’ˆå¯¹æ ‡æ³¨å®ä½“çš„ä»»åŠ¡ï¼Œæœ€å¥½ä¿è¯æ¯è¡Œæ•°æ®ä¸­æœ‰å®ä½“è¯ï¼ˆå³éå…¨Oçš„åºåˆ—ï¼‰

3. é¢„æµ‹ï¼šä¸åŒä»»åŠ¡æ¯ä¸€è¡Œå‡ä¸ºæ–‡æœ¬

### é…ç½®æ–‡ä»¶

train: è¡¨ç¤ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„å‚æ•°ï¼ŒåŒ…æ‹¬batchå¤§å°ï¼Œepochæ•°é‡ï¼Œè®­ç»ƒæ¨¡å¼ç­‰

data: è¡¨ç¤ºæ•°æ®é¢„å¤„ç†çš„å‚æ•°ï¼ŒåŒ…æ‹¬æœ€å¤§è¯æ•°å’Œå­—ç¬¦æ•°ï¼Œæ˜¯å¦ä½¿ç”¨è¯å†…éƒ¨å­—ç¬¦åºåˆ—ï¼Œæ˜¯å¦å¼€å¯åˆ†è¯

embed: è¯å‘é‡ï¼Œpreè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡

å‰©ä¸‹çš„æ¨¡å—å¯¹åº”ä¸åŒçš„æ¨¡å‹è¶…å‚æ•°

å…·ä½“ç»†èŠ‚å¯æŸ¥çœ‹é…ç½®æ–‡ä»¶æ³¨é‡Š

## æ¨¡å‹

### æ–‡æœ¬åˆ†ç±»

1. åŒå±‚åŒå‘LSTM + Attention ğŸ†—

    [DeepMoji](https://arxiv.org/abs/1708.00524)ä¸€æ–‡ä¸­æ‰€é‡‡ç”¨çš„çš„æ¨¡å‹æ¡†æ¶ï¼Œæœ¬ä»“åº“ä¸­å¯¹attentionå±‚ä½œäº†æ‰©å±•

    å¯¹åº”é…ç½®æ–‡ä»¶åç§°ï¼šbi_lstm_att

2. [Transformer](http://papers.nips.cc/paper/7181-attention-is-all-you-need) ğŸ†—

    é‡‡ç”¨Transformerä¸­çš„å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚æ¥è¡¨å¾æ–‡æœ¬ä¿¡æ¯ï¼Œè¯¦ç»†çš„ç»†èŠ‚å¯é˜…è¯»æ­¤[æ–‡ç« ](https://kexue.fm/archives/4765)

    å¯¹åº”é…ç½®æ–‡ä»¶åç§°ï¼šmulti_head_self_att

3. [TextCNN](https://arxiv.org/abs/1408.5882) ğŸ†—

    CNNç½‘ç»œä¹‹äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„å¼€å±±ä¹‹ä½œï¼Œåœ¨è¿‡å»å‡ å¹´ä¸­ç»å¸¸è¢«ç”¨ä½œbaselineï¼Œè¯¦ç»†çš„ç»†èŠ‚å¯é˜…è¯»æ­¤[æ–‡ç« ](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)

    å¯¹åº”é…ç½®æ–‡ä»¶åç§°ï¼štext_cnn

4. [DPCNN](http://www.aclweb.org/anthology/P17-1052)

    é€šè¿‡ä¸æ–­åŠ æ·±CNNç½‘ç»œæ¥è·å–æ›´å¥½çš„æ–‡æœ¬è¡¨å¾

5. [HAN](https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf)

    ä½¿ç”¨attentionæœºåˆ¶çš„æ–‡æ¡£åˆ†ç±»æ¨¡å‹

### åºåˆ—æ ‡æ³¨

1. [WordRNN](https://arxiv.org/abs/1707.06799) ğŸ†—

    Baselineæ¨¡å‹ï¼Œæ–‡æœ¬åºåˆ—ç»è¿‡åŒå‘LSTMåï¼Œç”±CRFå±‚ç¼–ç ä½œä¸ºè¾“å‡º

    å¯¹åº”é…ç½®æ–‡ä»¶åç§°ï¼šword_rnn

2. [CharRNN](https://pdfs.semanticscholar.org/b944/5206f592423f0b2faf05f99de124ccc6aaa8.pdf)

    åŸºäºæ±‰è¯­çš„ç‰¹ç‚¹ï¼Œåœ¨å­—ç¬¦çº§åˆ«çš„LSTMä¿¡æ¯å¤–ï¼Œæ‹¼æ¥åæ—éƒ¨é¦–ï¼Œåˆ†è¯ï¼ŒNgramä¿¡æ¯

3. [InnerChar](https://arxiv.org/abs/1611.04361) ğŸ†—

    åŸºäºå¦å¤–ä¸€ç¯‡[è®ºæ–‡](https://arxiv.org/abs/1511.08308)ï¼Œæ‰©å±•äº†æœ¬æ–‡çš„æ¨¡å‹ï¼Œä½¿ç”¨bi-lstmæˆ–CNNåœ¨è¯å†…éƒ¨çš„charçº§åˆ«è¿›è¡Œä¿¡æ¯çš„æŠ½å–ï¼Œç„¶åä¸åŸæ¥çš„è¯å‘é‡è¿›è¡Œconcatæˆ–attentionè®¡ç®—

    å¯¹åº”é…ç½®æ–‡ä»¶åç§°ï¼šword_rnnï¼Œå¹¶è®¾ç½®é…ç½®æ–‡ä»¶dataæ¨¡å—ä¸­çš„inner_charä¸ºTrue

4. [IDCNN](https://arxiv.org/abs/1702.02098) ğŸ†—

    è†¨èƒ€å·ç§¯ç½‘ç»œï¼Œåœ¨ä¿æŒå‚æ•°é‡ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå¢å¤§äº†å·ç§¯æ ¸çš„æ„Ÿå—é‡ï¼Œè¯¦ç»†çš„ç»†èŠ‚å¯é˜…è¯»æ­¤[æ–‡ç« ](http://www.crownpku.com//2017/08/26/%E7%94%A8IDCNN%E5%92%8CCRF%E5%81%9A%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E4%B8%AD%E6%96%87%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB.html)

    å¯¹åº”é…ç½®æ–‡ä»¶åç§°ï¼šidcnn

## æ€§èƒ½

### æ–‡æœ¬åˆ†ç±»

Model                   | 10-fold_f1   | Model Size   | Time per epoch
----------------------- | :------:     | :----------: | :-------------:
Bi-LSTM Attention       |              |              | 
Transformer             |              |              |
TextCNN                 |              |              |
DPCNN                   |              |              |
HAN                     |              |              |

### åºåˆ—æ ‡æ³¨

Model                   | 10-fold_f1   | Model Size   | Time per epoch
----------------------- | :------:     | :----------: | :-------------:
Baseline(WordRNN)       |              |              | 
WordRNN + InnerChar     |              |              |
CharRNN                 |              |              |
IDCNN                   |              |              |

## To-Doåˆ—è¡¨

1. å¥å­åˆ‡åˆ†æ¨¡å—

2. åŠ å…¥æ›´å¤šSOTAçš„æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒBERTï¼‰

3. å¢åŠ è¯­è¨€æ¨¡å‹çš„è®­ç»ƒ

4. æ”¯æŒè‡ªå®šä¹‰æ¨¡å—

5. ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆä¸€ä»½ä¸“å±çš„é…ç½®æ–‡ä»¶

## æ„Ÿè°¢

* æ•°æ®æµæ¨¡å—éƒ¨åˆ†ä»£ç å€Ÿé‰´äºæ­¤ï¼š https://github.com/Hironsan/anago/

* åºåˆ—æ ‡æ³¨ä»»åŠ¡çš„è¯„ä¼°å‡½æ•°æ¥æºäºæ­¤ï¼š https://github.com/chakki-works/seqeval
  
* bucketåºåˆ—åŒ–ä»£ç æ¥è‡ªï¼šhttps://github.com/tbennun/keras-bucketed-sequence

* å¤šå¤´æ³¨æ„åŠ›å±‚å’Œä½ç½®åµŒå…¥å±‚ä»£ç æ¥è‡ªï¼šhttps://github.com/bojone/attention

## è”ç³»æ–¹å¼

è”ç³»äººï¼šç‹å¥•ç£Š

ğŸ“§ é‚®ç®±ï¼šstevewyl@163.com

å¾®ä¿¡ï¼šSteve_1125

--------------------

# nlp_toolkit

Basic Chinese NLP Toolkits include following tasks, such as text classification, sequence labeling etc.

This repo reproduce some hot nlp papers in recent years. All the code is based on Keras.

Less than 10 lines of code, you can quickly train a text classfication model or sequence labeling model.

## Install

```bash
git clone https://github.com/stevewyl/nlp_toolkit
cd nlp_toolkit

# Use cpu-only
pip install -r requirements.txt

# Use GPU
pip install -r requirements-gpu.txt

# if keras_contrib install fail
pip install git+https://www.github.com/keras-team/keras-contrib.git
```

## Usage

The frameword of this repositoryï¼š

![framework](./images/framework.jpg)

Following modules are included inï¼š

1. Datasetï¼šText and label data are processed in a format suitable for model input. The main processing operations are cleaning, word segmentation and indexation.

2. Model Zoo & Layerï¼šThe collection of models commonly used in this task in recent years and some custom layers of Keras.
   
    Customized layers are as followed:

    * Attention

    * Multi-Head Attention

    * Position Embedding

3. Trainerï¼šDefine the training process of differnent models, which supports bucket sequence, customed callbacks and N-fold validation training.

    * Bucket Iterator: Accelerate model training by putting texts with similar lengths into the same batch to reduce the extra calculation of padding. In text classification task, it can help speed up RNN by over 2 times. (currently not support for networks with Flatten layer)

    * callbacks: The training process is controlled by custom callbacks. Currently, the preset callbacks include early stopping strategy, automatical learning rate decay, richer evaluation functions and etc.

    * N-fold cross validation: Support cross-validation to test the true capabilities of the model.

4. Classifier & Sequence Labelerï¼šEncapsulates classes that support different training tasks.

Quick startï¼š

```python
from nlp_toolkit import Dataset, Classifier, Labeler
import yaml

config = yaml.load(open('your_config.yaml'))

# text classification task
dataset = Dataset(fname='your_data.txt', task_type='classification', mode='train', config=config)
x, y, config = dataset.transform()
text_classifier = Classifier(config=config, model_name='multi_head_self_att', seq_type='bucket', transformer=dataset.transformer)
trained_model = text_classifier.train(x, y)

# sequence labeling task
dataset = Dataset(fname='your_data.txt', task_type='seq_label', mode='train', config=config)
x, y, config = dataset.transform()
seq_labeler = Labeler(config=config, model_name='word_rnn', seq_type='bucket',,transformer=dataset.transformer)
trained_model = seq_labeler.train(x, y)

# predict
trained_model
```

For more details, please read the jupyter notebooks in **examples** folder

### Data Format


## Models

### Text Classification

### Sequence Labeling

## Performance

Here list the performace based on following two datasets:

1. Company Pros and Cons: Crawled from Kanzhun.com and Dajie.com, it contains 95K reviews on the pros and cons of different companies.
2. 

### Text Classification

Model                   | 10-fold_f1   | Model Size   | Time per epoch
----------------------- | :------:     | :----------: | :-------------:
Bi-LSTM Attention       |              |              | 
Transformer             |              |              |
TextCNN                 |              |              |
DPCNN                   |              |              |
HAN                     |              |              |

### Sequence Labeling

Model                   | 10-fold_f1   | Model Size   | Time per epoch
----------------------- | :------:     | :----------: | :-------------:
Baseline(WordRNN)       |              |              | 
WordRNN + InnerChar     |              |              |
CharRNN                 |              |              |
IDCNN                   |              |              |

## To-Do List

1. Sentence split module

2. Add more SOTA model(such as BERT)

3. Support for training language model

4. Support for customized moudle

5. Generate a unique configuration file for each model

## Acknowledgments

* The preprocessor part is derived from https://github.com/Hironsan/anago/
* The evaluations for sequence labeling are based on a modified version of https://github.com/chakki-works/seqeval
* Bucket sequence are based on https://github.com/tbennun/keras-bucketed-sequence
* Multi-head attention and position embedding are from: https://github.com/bojone/attention

## Contact
Contact: Yilei Wang

E-mail: stevewyl@163.com

WeChat: Steve_1125